{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "from ipywidgets import IntProgress\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 2. conv block\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 3. conv block\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 4. conv block\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(320,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        # conv embedding\n",
    "        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "        # transformer embedding\n",
    "        x_reduced = self.transf_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(2,0,1) # requires shape = (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([conv_embedding, transf_embedding], dim=1) \n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n",
    "SAMPLE_RATE = 48000\n",
    "AUDIO_PATH = '/Users/winsteadx/Desktop/Podcast/podcasts-segments/all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model is loaded from /Users/winsteadx/Desktop/audio_process/feature_extraction/cnn_transf_parallel_model.pt\n"
     ]
    }
   ],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'/Users/winsteadx/Desktop/audio_process/feature_extraction')\n",
    "model = ParallelModel(8)\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_transf_parallel_model.pt'), map_location=torch.device('cpu')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_transf_parallel_model.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(filename, AUDIO_PATH, SAMPLE_RATE, model):\n",
    "    try:\n",
    "        filepath = AUDIO_PATH + filename\n",
    "        y, sr = librosa.load(filepath, sr=SAMPLE_RATE)\n",
    "        \n",
    "        signals = []\n",
    "        # segments shorter than 2 seconds\n",
    "        if y.shape[0] < (48000*2):\n",
    "            signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "            audio = y[:]\n",
    "            signal[:len(audio)] = audio\n",
    "            signals.append(signal)\n",
    "\n",
    "        for i in range(y.shape[0]//48000 - 1):\n",
    "            signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "            audio = y[i*48000:(i+3)*48000]\n",
    "            signal[:len(audio)] = audio\n",
    "            signals.append(signal)\n",
    "        signals = np.stack(signals,axis=0)\n",
    "\n",
    "        X_test = signals\n",
    "\n",
    "        mel_test = []\n",
    "        #print(\"Calculatin mel spectrograms for test set\")\n",
    "        for i in range(X_test.shape[0]):\n",
    "            mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "            mel_test.append(mel_spectrogram)\n",
    "            #print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "        #print('')\n",
    "        mel_test = np.stack(mel_test,axis=0)\n",
    "        del X_test\n",
    "        X_test = mel_test\n",
    "        #print(f'X_test:{X_test.shape}')\n",
    "\n",
    "        X_test = np.expand_dims(X_test, 1)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        b,c,h,w = X_test.shape\n",
    "        X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_test = np.reshape(X_test, newshape=(b,c,h,w))\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax = model(X_test_tensor)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "        \n",
    "        predictions = predictions.tolist()\n",
    "        softmaxs = output_softmax.tolist()\n",
    "\n",
    "        EMOTIONS_counter = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 0:0}\n",
    "        for pred in predictions:\n",
    "            EMOTIONS_counter[pred] += 1\n",
    "        pred_result = max(EMOTIONS_counter.items(), key=operator.itemgetter(1))[0]\n",
    "        pred_emotion = EMOTIONS[pred_result]\n",
    "\n",
    "        softmax = 0\n",
    "        for sf in softmaxs:\n",
    "            softmax += sf[pred_result]\n",
    "        \n",
    "        return pred_emotion, softmax\n",
    "    # audio length = 0\n",
    "    except ValueError:\n",
    "        return 'neutral', 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_features = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 68337/68337 [28:10:20<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir(AUDIO_PATH)):\n",
    "    if filename.endswith(\".wav\"):\n",
    "         show, show_name, ep_name, seg_num = filename.split('_')\n",
    "         seg_num = int(seg_num.split('.')[0])\n",
    "         \n",
    "         emotion, softmax = predict_emotion(filename, AUDIO_PATH=AUDIO_PATH, SAMPLE_RATE=SAMPLE_RATE, model=model)\n",
    "         \n",
    "         ep_features[ep_name][seg_num]['paral_CNN_transf_emotion'] = emotion\n",
    "         ep_features[ep_name][seg_num]['paral_CNN_transf_softmax'] = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/winsteadx/Desktop/audio_process/feature_extraction/episodes_complex_v8_6_0-3.json', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 855/855 [00:00<00:00, 11950.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for ep_dict in tqdm(json_file['episodes_speaker']):\n",
    "    ep_name = ep_dict['episode_name']\n",
    "    for i in range(len(ep_dict['transcripts'])):\n",
    "        trans_dict = ep_dict['transcripts'][i]\n",
    "        \n",
    "        trans_dict['paral_CNN_transf_emotion'] = ep_features[ep_name][i]['paral_CNN_transf_emotion']\n",
    "        trans_dict['paral_CNN_transf_softmax'] = ep_features[ep_name][i]['paral_CNN_transf_softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/winsteadx/Desktop/audio_process/feature_extraction/episodes_complex_v9_6_0-3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_file, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}